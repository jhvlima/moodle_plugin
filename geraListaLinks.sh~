#!/bin/bash

#
# Web scraper que gera uma lista  de links de pdf de decisoes judiciais
#

# Link das base de dados
#dataSet_1="http://eliasdeoliveira.com.br/seminars/pagina-rouboSimples.html"

# Obtem o conteudo html (fetch) da pagina do data set
#htmlContent=$(curl -s "$dataSet_1")

# Obtem a lista de todos os nomes dos PDFs da pagina
#pdfNames=$(echo "$htmlContent" | grep -oP '(?<=./tmp/)[^"]*(?=.pdf)')

pdfNames=$(cat processosAnotados.txt)
# Cria o arquivo para receber os links
arquivo="listaLinksAnotados.txt"
touch $arquivo

# Define o limite para o numero de PDFs
limit=10
count=0

# imprime no arquivo
for pdfName in $pdfNames; do
    if [ $count -ge $limit ]; then
        break
    fi
    echo http://eliasdeoliveira.com.br/seminars/tmp/${pdfName}.pdf >>$arquivo
    #((count++))
done



# 
# colocar links aleatorios no csv das respostas
#

# Source and target files
#SOURCE_FILE="listaLinks.txt"
#TARGET_CSV="notastreino.csv"
#TEMP_FILE="temp_target.csv"

# Get all lines from the source file and shuffle them
#SHUFFLED_LINES=$(shuf "$SOURCE_FILE")

# Get the number of lines in the target CSV
#NUM_LINES=$(wc -l < "$TARGET_CSV")

# Add a new column to each line by pasting the shuffled lines
#paste -d ';' "$TARGET_CSV" <(echo "$SHUFFLED_LINES" | head -n "$NUM_LINES") > "$TEMP_FILE"

# Replace the target CSV with the updated one
#mv "$TEMP_FILE" "$TARGET_CSV"


#
# pegar o conteudo das respostas dos alunos 
#
grep '<p>' content.html | sed 's/<[^>]*>//g' > respostaAluno.txt
